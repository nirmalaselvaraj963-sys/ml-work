{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoWsn/UX9TEXgYnf5jFbyL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirmalaselvaraj963-sys/ml-work/blob/main/ann%2Ccnn%2Clstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "33sFIwMmkj0O"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Paths to your zip files\n",
        "melody_zip = \"/content/melody songs.zip\"\n",
        "rap_zip = \"/content/Rap song.zip\"\n",
        "\n",
        "# Extract them\n",
        "with zipfile.ZipFile(melody_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dataset/Melody\")\n",
        "\n",
        "with zipfile.ZipFile(rap_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/dataset/Rap\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for folder in [\"/content/audio_dataset/melody\", \"/content/audio_dataset/rap\"]:\n",
        "    print(f\"\\nüìÇ Checking: {folder}\")\n",
        "    if os.path.exists(folder):\n",
        "        files = os.listdir(folder)\n",
        "        print(f\"Total files: {len(files)}\")\n",
        "        for f in files[:10]:\n",
        "            print(\"  ‚Üí\", f)\n",
        "    else:\n",
        "        print(\"‚ùå Folder does not exist\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jtmxJ0pVbcq",
        "outputId": "abb2484f-07b3-43cf-dad4-afed3d00040b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìÇ Checking: /content/audio_dataset/melody\n",
            "Total files: 1\n",
            "  ‚Üí melody songs\n",
            "\n",
            "üìÇ Checking: /content/audio_dataset/rap\n",
            "Total files: 1\n",
            "  ‚Üí Rap song\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# Remove any old extraction folders\n",
        "for folder in [\"/content/audio_dataset\", \"/content/temp_extract\"]:\n",
        "    if os.path.exists(folder):\n",
        "        shutil.rmtree(folder)\n",
        "\n",
        "print(\"üßπ Cleaned old folders successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcO-u-vGWKpB",
        "outputId": "e934e57b-13ac-4119-e40d-01e15c8a69e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Cleaned old folders successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile, os, shutil  #cnn\n",
        "\n",
        "def extract_nested_zip(zip_path, target_folder):\n",
        "    temp_dir = \"/content/temp_extract\"\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(temp_dir)\n",
        "\n",
        "    # Move all audio files (.mp3/.wav)\n",
        "    os.makedirs(target_folder, exist_ok=True)\n",
        "    for root, _, files in os.walk(temp_dir):\n",
        "        for f in files:\n",
        "            if f.lower().endswith(('.mp3', '.wav')):\n",
        "                shutil.move(os.path.join(root, f), os.path.join(target_folder, f))\n",
        "    shutil.rmtree(temp_dir)\n",
        "\n",
        "# Base dataset folder\n",
        "os.makedirs(\"/content/audio_dataset\", exist_ok=True)\n",
        "\n",
        "# Your uploaded ZIP paths (change names if needed)\n",
        "rap_zip = \"/content/Rap song.zip\"\n",
        "melody_zip = \"/content/melody songs.zip\"\n",
        "\n",
        "# Extract both\n",
        "extract_nested_zip(rap_zip, \"/content/audio_dataset/rap\")\n",
        "extract_nested_zip(melody_zip, \"/content/audio_dataset/melody\")\n",
        "\n",
        "print(\"‚úÖ Extracted and organized all audio files correctly!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vknPA4T8WRDp",
        "outputId": "b9026076-9b3c-4de4-f708-2ebd8d3cae81"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extracted and organized all audio files correctly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/audio_dataset -type f | grep -E \".mp3|.wav\"\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYG8kh3sXmLh",
        "outputId": "70ccdbfc-e8d7-435c-9f01-939a1bab9ebc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/audio_dataset/rap/Kappe-Varroh.mp3\n",
            "/content/audio_dataset/rap/170CM.mp3\n",
            "/content/audio_dataset/rap/Engeyum Eppothum.mp3\n",
            "/content/audio_dataset/rap/Champagini.mp3\n",
            "/content/audio_dataset/rap/Naan Kudikka Poren.mp3\n",
            "/content/audio_dataset/rap/Madai-Thiranthu-MassTamilan.com.mp3\n",
            "/content/audio_dataset/rap/Ini-Illaye-Hum.mp3\n",
            "/content/audio_dataset/rap/Sollu Thamizhan (Somberi) - Havoc Brothers  Official Lyrics Video.mp3\n",
            "/content/audio_dataset/rap/Rekka-Rekka-MassTamilan.dev.mp3\n",
            "/content/audio_dataset/rap/Paiya Dei.mp3\n",
            "/content/audio_dataset/rap/Aathichudi.mp3\n",
            "/content/audio_dataset/rap/His Name is John.mp3\n",
            "/content/audio_dataset/rap/AK - The Tiger.mp3\n",
            "/content/audio_dataset/rap/Porkkalam-Tamil-Rap.mp3\n",
            "/content/audio_dataset/rap/Vaadi-Pulla-Vaadi.mp3\n",
            "/content/audio_dataset/rap/Semma-Weightu-MassTamilan.io.mp3\n",
            "/content/audio_dataset/rap/Club-La-Mabula.mp3\n",
            "/content/audio_dataset/rap/Pakkam-Vanthu.mp3\n",
            "/content/audio_dataset/rap/Magudi-Magudi.mp3\n",
            "/content/audio_dataset/rap/Kaathu Mela.mp3\n",
            "/content/audio_dataset/rap/VETRIVEL.mp3\n",
            "/content/audio_dataset/melody/Vishwaroopam - Unnai Kaanadhu Naan Video  Kamal Haasan.mp3\n",
            "/content/audio_dataset/melody/Yeya En Kottikkaaraa Video Song  Papanasam  Kamal Haasan  Gautami  Jeethu Joseph  Ghibran.mp3\n",
            "/content/audio_dataset/melody/Pariyerum Perumal  Potta Kaatil Poovasam Video Song  Kathir, Anandhi  Santhosh Narayanan.mp3\n",
            "/content/audio_dataset/melody/Un Kanney Aayiram Kadha Pesudhey  Pradeep Kumar  SuperSinger  - Kulam.mp3\n",
            "/content/audio_dataset/melody/Maamannan - Nenjame Nenjame Video  Udhayanidhi Stalin  Vadivelu  A.R Rahman.mp3\n",
            "/content/audio_dataset/melody/Varaha Nadhikarai (From Sangamam).mp3\n",
            "/content/audio_dataset/melody/Aasai Oru Pulveli Video Song  Attakathi  Dinesh, Nandita Swetha  Santhosh Narayanan  Pa. Ranjith.mp3\n",
            "/content/audio_dataset/melody/Maragatha Naanayam  Nee Kavithaigala Song with Lyrics  Aadhi, Nikki Galrani  Dhibu Ninan Thomas.mp3\n",
            "/content/audio_dataset/melody/The Romance Of Power Paandi - Venpani Malare ft. Dhanush [Lyric Video]  Power Paandi  Sean Roldan.mp3\n",
            "/content/audio_dataset/melody/Raavanan - Usure Pogudhey Video  A.R. Rahman  Vikram, Aishwarya Rai.mp3\n",
            "/content/audio_dataset/melody/Kadal - Nenjukkule Video  A.R. Rahman.mp3\n",
            "/content/audio_dataset/melody/Manithan - Aval Video  Udhayanidhi  Santhosh Narayanan.mp3\n",
            "/content/audio_dataset/melody/Petta - Ilamai Thirumbudhe Official Video (Tamil)  Rajinikanth, Simran  Anirudh Ravichander.mp3\n",
            "/content/audio_dataset/melody/Kadhal Kanave Official Full Song - Mundasupatti.mp3\n",
            "/content/audio_dataset/melody/Anju Vanna Poove (Reprise) - Video  Thug Life  Kamal Haasan  Mani Ratnam  STR  AR Rahman.mp3\n",
            "/content/audio_dataset/melody/Virumaandi Movie Songs  Onnavida indha song  Kamal Haasan  Abhirami  Nassar  Ilaiyaraaja.mp3\n",
            "/content/audio_dataset/melody/Pannaiyaarum Padminiyum - Onakkaaga Poranthaenae Video  Vijay Sethupathi.mp3\n",
            "/content/audio_dataset/melody/Anegan - Aathadi Aathadi Lyric  Dhanush  Harris Jayaraj.mp3\n",
            "/content/audio_dataset/melody/Irudhi Suttru Video Songs  Ey Sandakaara Full Video SongR Madhavan,Ritika SinghSanthosh Narayanan.mp3\n",
            "/content/audio_dataset/melody/Guru (Tamil) - Aaruyirae Video  A.R. Rahman.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: CNN for Song Classification\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Path to dataset (replace if different)\n",
        "data_dir = \"/content/audio_dataset\" # Changed from /content/dataset\n",
        "\n",
        "# Parameters\n",
        "sample_rate = 22050\n",
        "duration = 30  # seconds\n",
        "samples_per_track = sample_rate * duration\n",
        "\n",
        "# Helper function to extract MFCC features\n",
        "def extract_features(file_path, n_mfcc=40):\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, duration=30)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "        return mfcc\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load dataset\n",
        "X, y = [], []\n",
        "labels = sorted(os.listdir(data_dir))  # Example: ['melody', 'rap']\n",
        "\n",
        "print(\"üéß Loading dataset and extracting features...\")\n",
        "print(f\"Looking for labels in: {data_dir}\") # Debugging print\n",
        "for label_idx, label in enumerate(labels):\n",
        "    folder_path = os.path.join(data_dir, label)\n",
        "    print(f\"Checking folder: {folder_path}\") # Debugging print\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(f\"Skipping non-directory: {folder_path}\") # Debugging print\n",
        "        continue\n",
        "    print(f\"Processing label: {label}\") # Debugging print\n",
        "    file_count = 0 # Debugging counter\n",
        "    for file_name in tqdm(os.listdir(folder_path), desc=f\"Processing {label}\"):\n",
        "        if file_name.lower().endswith((\".mp3\", \".wav\")):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            # print(f\"Attempting to extract features from: {file_path}\") # Debugging print (can be noisy)\n",
        "            mfcc = extract_features(file_path)\n",
        "            if mfcc is not None:\n",
        "                X.append(mfcc)\n",
        "                y.append(label_idx)\n",
        "                file_count += 1 # Debugging counter\n",
        "    print(f\"Finished processing label {label}, found {file_count} audio files.\") # Debugging print\n",
        "\n",
        "\n",
        "# Convert lists to arrays\n",
        "if len(X) == 0:\n",
        "    raise ValueError(\"No data loaded. Please check the dataset directory and file formats.\")\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X = X[..., np.newaxis]  # Add channel dimension\n",
        "y = to_categorical(y, num_classes=len(labels))  # ‚úÖ fixed line\n",
        "\n",
        "print(f\"‚úÖ Data loaded successfully! Shape: {X.shape}, Labels: {y.shape}\")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(labels), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"üöÄ Training CNN model...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=16, # Added closing parenthesis and batch_size\n",
        "    verbose=1 # Added verbose for training progress\n",
        ") # Added closing parenthesis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GiBRfoTyXOBp",
        "outputId": "37c80a6c-82fb-4a96-ca98-e975cd9b7c6c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéß Loading dataset and extracting features...\n",
            "Looking for labels in: /content/audio_dataset\n",
            "Checking folder: /content/audio_dataset/melody\n",
            "Processing label: melody\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing melody: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing label melody, found 20 audio files.\n",
            "Checking folder: /content/audio_dataset/rap\n",
            "Processing label: rap\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:03<00:00,  6.48it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing label rap, found 21 audio files.\n",
            "‚úÖ Data loaded successfully! Shape: (41, 40, 1292, 1), Labels: (41, 2)\n",
            "üöÄ Training CNN model...\n",
            "Epoch 1/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.5417 - loss: 77.4874\n",
            "Epoch 2/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.5208 - loss: 458.1194\n",
            "Epoch 3/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.6458 - loss: 81.5623\n",
            "Epoch 4/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 0.5208 - loss: 145.9682\n",
            "Epoch 5/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 77.2188\n",
            "Epoch 6/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.8542 - loss: 3.5010\n",
            "Epoch 7/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.7708 - loss: 6.0600\n",
            "Epoch 8/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.7500 - loss: 4.8551\n",
            "Epoch 9/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.6667 - loss: 3.3537\n",
            "Epoch 10/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.7708 - loss: 2.7087\n",
            "Epoch 11/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.8750 - loss: 0.5686\n",
            "Epoch 12/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9583 - loss: 0.1079\n",
            "Epoch 13/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.8958 - loss: 0.2878\n",
            "Epoch 14/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.9167 - loss: 0.1982\n",
            "Epoch 15/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.9375 - loss: 0.2208\n",
            "Epoch 16/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9792 - loss: 0.0549\n",
            "Epoch 17/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.9375 - loss: 0.1489\n",
            "Epoch 18/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0644\n",
            "Epoch 19/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.9167 - loss: 0.0909\n",
            "Epoch 20/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.9583 - loss: 0.0599\n",
            "Epoch 21/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0281\n",
            "Epoch 22/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0079\n",
            "Epoch 23/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0088\n",
            "Epoch 24/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0132\n",
            "Epoch 25/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0149\n",
            "Epoch 26/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.9583 - loss: 0.1771\n",
            "Epoch 27/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0324\n",
            "Epoch 28/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0145\n",
            "Epoch 29/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0237\n",
            "Epoch 30/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üéØ Evaluate model on test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\n‚úÖ Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "print(f\"üßÆ Test Loss: {test_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_LQi4j_YiEh",
        "outputId": "e09f8474-a54e-41c5-fadb-3c245d5bb3af"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Test Accuracy: 66.67%\n",
            "üßÆ Test Loss: 0.5380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üéµ Song Classification using ANN\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ‚úÖ Path to dataset\n",
        "data_dir = \"/content/audio_dataset\"  # contains folders like /rap and /melody\n",
        "\n",
        "# üéß Feature extraction function\n",
        "def extract_features(file_path, n_mfcc=40):\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, duration=30)\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "        mfcc_scaled = np.mean(mfcc.T, axis=0)  # Average over time\n",
        "        return mfcc_scaled\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# üéµ Load dataset\n",
        "X, y = [], []\n",
        "labels = sorted(os.listdir(data_dir))\n",
        "\n",
        "print(\"üéß Loading dataset and extracting features...\")\n",
        "for label in labels:\n",
        "    folder_path = os.path.join(data_dir, label)\n",
        "    if not os.path.isdir(folder_path):\n",
        "        continue\n",
        "    for file_name in tqdm(os.listdir(folder_path), desc=f\"Processing {label}\"):\n",
        "        if file_name.lower().endswith((\".mp3\", \".wav\")):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            features = extract_features(file_path)\n",
        "            if features is not None:\n",
        "                X.append(features)\n",
        "                y.append(label)\n",
        "\n",
        "# üßÆ Convert lists to arrays\n",
        "if len(X) == 0:\n",
        "    raise ValueError(\"No data loaded. Please check the dataset directory and file formats.\")\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# üî¢ Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# ‚öôÔ∏è Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# üß† Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_categorical, test_size=0.2, random_state=42, stratify=y_categorical\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset ready! X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "\n",
        "# üèóÔ∏è ANN model\n",
        "model = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(y_categorical.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "# ‚öôÔ∏è Compile model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# üöÄ Train model\n",
        "print(\"üöÄ Training ANN model...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    validation_data=(X_test, y_test),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# üéØ Evaluate model\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nüéµ ANN Test Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "# üíæ Save model\n",
        "model.save(\"/content/song_ann_model.h5\")\n",
        "print(\"üíæ Model saved successfully at /content/song_ann_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z19oznROYlpw",
        "outputId": "0948640f-f334-4e8b-e414-8eaec09263e8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéß Loading dataset and extracting features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing melody: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  6.68it/s]\n",
            "Processing rap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:04<00:00,  5.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset ready! X_train: (32, 40), y_train: (32, 2)\n",
            "üöÄ Training ANN model...\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 240ms/step - accuracy: 0.5417 - loss: 0.7564 - val_accuracy: 0.3333 - val_loss: 0.7471\n",
            "Epoch 2/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5000 - loss: 0.7122 - val_accuracy: 0.4444 - val_loss: 0.7154\n",
            "Epoch 3/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6250 - loss: 0.6292 - val_accuracy: 0.5556 - val_loss: 0.6912\n",
            "Epoch 4/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8125 - loss: 0.5257 - val_accuracy: 0.6667 - val_loss: 0.6679\n",
            "Epoch 5/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8542 - loss: 0.4985 - val_accuracy: 0.6667 - val_loss: 0.6534\n",
            "Epoch 6/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8333 - loss: 0.4648 - val_accuracy: 0.6667 - val_loss: 0.6386\n",
            "Epoch 7/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8542 - loss: 0.4241 - val_accuracy: 0.6667 - val_loss: 0.6276\n",
            "Epoch 8/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9583 - loss: 0.3574 - val_accuracy: 0.6667 - val_loss: 0.6162\n",
            "Epoch 9/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8542 - loss: 0.3841 - val_accuracy: 0.6667 - val_loss: 0.6072\n",
            "Epoch 10/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8750 - loss: 0.3548 - val_accuracy: 0.6667 - val_loss: 0.6000\n",
            "Epoch 11/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9167 - loss: 0.3419 - val_accuracy: 0.7778 - val_loss: 0.5934\n",
            "Epoch 12/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9792 - loss: 0.2758 - val_accuracy: 0.7778 - val_loss: 0.5938\n",
            "Epoch 13/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9375 - loss: 0.2072 - val_accuracy: 0.7778 - val_loss: 0.5973\n",
            "Epoch 14/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9583 - loss: 0.1946 - val_accuracy: 0.7778 - val_loss: 0.6049\n",
            "Epoch 15/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9792 - loss: 0.1658 - val_accuracy: 0.6667 - val_loss: 0.6163\n",
            "Epoch 16/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9792 - loss: 0.1699 - val_accuracy: 0.6667 - val_loss: 0.6249\n",
            "Epoch 17/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.1330 - val_accuracy: 0.6667 - val_loss: 0.6320\n",
            "Epoch 18/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.1376 - val_accuracy: 0.6667 - val_loss: 0.6438\n",
            "Epoch 19/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.1001 - val_accuracy: 0.6667 - val_loss: 0.6611\n",
            "Epoch 20/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9583 - loss: 0.1042 - val_accuracy: 0.6667 - val_loss: 0.6839\n",
            "Epoch 21/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0662 - val_accuracy: 0.6667 - val_loss: 0.7098\n",
            "Epoch 22/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0749 - val_accuracy: 0.6667 - val_loss: 0.7363\n",
            "Epoch 23/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0613 - val_accuracy: 0.6667 - val_loss: 0.7670\n",
            "Epoch 24/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0294 - val_accuracy: 0.6667 - val_loss: 0.7955\n",
            "Epoch 25/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0243 - val_accuracy: 0.6667 - val_loss: 0.8234\n",
            "Epoch 26/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.0405 - val_accuracy: 0.6667 - val_loss: 0.8527\n",
            "Epoch 27/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0362 - val_accuracy: 0.6667 - val_loss: 0.8806\n",
            "Epoch 28/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.0117 - val_accuracy: 0.6667 - val_loss: 0.9059\n",
            "Epoch 29/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0209 - val_accuracy: 0.6667 - val_loss: 0.9290\n",
            "Epoch 30/30\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0166 - val_accuracy: 0.6667 - val_loss: 0.9494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéµ ANN Test Accuracy: 66.67%\n",
            "üíæ Model saved successfully at /content/song_ann_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm # Import tqdm for progress bars\n",
        "\n",
        "# --- Step 1: Load Dataset ---\n",
        "data_dir = \"/content/audio_dataset\"  # Corrected path to where files were extracted\n",
        "genres = os.listdir(data_dir)\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "print(f\"Loading dataset from: {data_dir}\") # Debugging print\n",
        "for genre in genres:\n",
        "    genre_dir = os.path.join(data_dir, genre)\n",
        "    print(f\"Checking directory: {genre_dir}\") # Debugging print\n",
        "    if not os.path.isdir(genre_dir):\n",
        "        print(f\"Skipping non-directory: {genre_dir}\") # Debugging print\n",
        "        continue\n",
        "    print(f\"Processing genre: {genre}\") # Debugging print\n",
        "    file_count = 0 # Debugging counter\n",
        "    for file in tqdm(os.listdir(genre_dir), desc=f\"Processing {genre}\"): # Added tqdm\n",
        "        file_path = os.path.join(genre_dir, file)\n",
        "        if file.lower().endswith(('.mp3', '.wav')): # Check for audio file extensions\n",
        "            try:\n",
        "                y, sr = librosa.load(file_path, duration=30)  # load 30 sec\n",
        "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "                mfcc = np.mean(mfcc.T, axis=0)  # average over time\n",
        "                features.append(mfcc)\n",
        "                labels.append(genres.index(genre))\n",
        "                file_count += 1 # Debugging counter\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error loading {file_path}: {e}\") # Changed print format\n",
        "    print(f\"Finished processing genre {genre}, found {file_count} audio files.\") # Debugging print\n",
        "\n",
        "\n",
        "features = np.array(features)\n",
        "labels = np.array(labels)\n",
        "\n",
        "if len(features) == 0: # Check if any features were loaded\n",
        "    raise ValueError(\"No audio features loaded. Please check the dataset directory and file formats.\")\n",
        "\n",
        "# --- Step 2: Prepare Data for LSTM ---\n",
        "# LSTM expects 3D input: (samples, timesteps, features)\n",
        "features = np.expand_dims(features, axis=1)  # (samples, 1, features)\n",
        "labels = to_categorical(labels)  # one-hot encoding\n",
        "\n",
        "print(f\"‚úÖ Data loaded successfully! Features shape: {features.shape}, Labels shape: {labels.shape}\") # Debugging print\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Step 3: Build LSTM Model ---\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(len(genres), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# --- Step 4: Train Model ---\n",
        "print(\"üöÄ Training LSTM model...\") # Debugging print\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test), verbose=1) # Added verbose\n",
        "\n",
        "# --- Step 5: Evaluate ---\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0) # Added verbose\n",
        "print(f\"üéØ Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# --- Step 6: Save Model (Optional) ---\n",
        "# model.save(\"/content/song_lstm_model.h5\")\n",
        "# print(\"üíæ Model saved successfully at /content/song_lstm_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mcwJBC6aZgHI",
        "outputId": "d022f17e-3021-493c-9cf0-9665fc696699"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: /content/audio_dataset\n",
            "Checking directory: /content/audio_dataset/rap\n",
            "Processing genre: rap\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:03<00:00,  6.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing genre rap, found 21 audio files.\n",
            "Checking directory: /content/audio_dataset/melody\n",
            "Processing genre: melody\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing melody: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.47it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished processing genre melody, found 20 audio files.\n",
            "‚úÖ Data loaded successfully! Features shape: (41, 1, 40), Labels shape: (41, 2)\n",
            "üöÄ Training LSTM model...\n",
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - accuracy: 0.5083 - loss: 0.6933 - val_accuracy: 0.6667 - val_loss: 0.6634\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7250 - loss: 0.6642 - val_accuracy: 0.8889 - val_loss: 0.6523\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6500 - loss: 0.6596 - val_accuracy: 0.6667 - val_loss: 0.6449\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7708 - loss: 0.6114 - val_accuracy: 0.6667 - val_loss: 0.6331\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8833 - loss: 0.5731 - val_accuracy: 0.6667 - val_loss: 0.6206\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7833 - loss: 0.5885 - val_accuracy: 0.6667 - val_loss: 0.6020\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8292 - loss: 0.5601 - val_accuracy: 0.6667 - val_loss: 0.5969\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8542 - loss: 0.5033 - val_accuracy: 0.6667 - val_loss: 0.5909\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8625 - loss: 0.4781 - val_accuracy: 0.6667 - val_loss: 0.5880\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9333 - loss: 0.4209 - val_accuracy: 0.6667 - val_loss: 0.5862\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8875 - loss: 0.3853 - val_accuracy: 0.7778 - val_loss: 0.5943\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9250 - loss: 0.3219 - val_accuracy: 0.7778 - val_loss: 0.6091\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9417 - loss: 0.2646 - val_accuracy: 0.7778 - val_loss: 0.6408\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9208 - loss: 0.2886 - val_accuracy: 0.7778 - val_loss: 0.6375\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9083 - loss: 0.3265 - val_accuracy: 0.7778 - val_loss: 0.6260\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9542 - loss: 0.1804 - val_accuracy: 0.6667 - val_loss: 0.6843\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9542 - loss: 0.1869 - val_accuracy: 0.6667 - val_loss: 0.7757\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9292 - loss: 0.1645 - val_accuracy: 0.6667 - val_loss: 0.8176\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9667 - loss: 0.1106 - val_accuracy: 0.6667 - val_loss: 0.7694\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9417 - loss: 0.1213 - val_accuracy: 0.7778 - val_loss: 0.7563\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9667 - loss: 0.0956 - val_accuracy: 0.7778 - val_loss: 0.7806\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0598 - val_accuracy: 0.6667 - val_loss: 0.8520\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0446 - val_accuracy: 0.6667 - val_loss: 0.9356\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0391 - val_accuracy: 0.6667 - val_loss: 0.9736\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0314 - val_accuracy: 0.6667 - val_loss: 0.9846\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0162 - val_accuracy: 0.6667 - val_loss: 0.9764\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0151 - val_accuracy: 0.6667 - val_loss: 0.9711\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0224 - val_accuracy: 0.6667 - val_loss: 1.0011\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0153 - val_accuracy: 0.6667 - val_loss: 1.0901\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0152 - val_accuracy: 0.6667 - val_loss: 1.1462\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0150 - val_accuracy: 0.6667 - val_loss: 1.1556\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0167 - val_accuracy: 0.6667 - val_loss: 1.1815\n",
            "Epoch 33/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0118 - val_accuracy: 0.6667 - val_loss: 1.1930\n",
            "Epoch 34/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.6667 - val_loss: 1.1942\n",
            "Epoch 35/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0085 - val_accuracy: 0.6667 - val_loss: 1.1784\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.6667 - val_loss: 1.1675\n",
            "Epoch 37/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0117 - val_accuracy: 0.6667 - val_loss: 1.1350\n",
            "Epoch 38/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.7778 - val_loss: 1.1410\n",
            "Epoch 39/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.8889 - val_loss: 1.1372\n",
            "Epoch 40/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0091 - val_accuracy: 0.6667 - val_loss: 1.2010\n",
            "Epoch 41/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.6667 - val_loss: 1.2949\n",
            "Epoch 42/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0044 - val_accuracy: 0.6667 - val_loss: 1.3632\n",
            "Epoch 43/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.6667 - val_loss: 1.3957\n",
            "Epoch 44/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.6667 - val_loss: 1.4008\n",
            "Epoch 45/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.6667 - val_loss: 1.4049\n",
            "Epoch 46/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.6667 - val_loss: 1.4010\n",
            "Epoch 47/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0132 - val_accuracy: 0.6667 - val_loss: 1.4365\n",
            "Epoch 48/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.6667 - val_loss: 1.5153\n",
            "Epoch 49/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.6667 - val_loss: 1.5623\n",
            "Epoch 50/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 0.6667 - val_loss: 1.4644\n",
            "üéØ Test Accuracy: 66.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm # Import tqdm for progress bars\n",
        "\n",
        "# --- Step 1: Load Dataset ---\n",
        "data_dir = \"/content/audio_dataset\"  # Corrected path\n",
        "genres = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "print(f\"Loading dataset from: {data_dir}\") # Debugging print\n",
        "print(f\"Found genres: {genres}\") # Debugging print\n",
        "\n",
        "for genre in genres:\n",
        "    genre_dir = os.path.join(data_dir, genre)\n",
        "    print(f\"Checking directory: {genre_dir}\") # Debugging print\n",
        "    for file in tqdm(os.listdir(genre_dir), desc=f\"Processing {genre}\"): # Added tqdm\n",
        "        file_path = os.path.join(genre_dir, file)\n",
        "        if file.lower().endswith(('.mp3', '.wav')): # Check for audio file extensions\n",
        "            try:\n",
        "                y, sr = librosa.load(file_path, duration=30)\n",
        "                mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "                mfcc = mfcc.T  # transpose to (timesteps, features)\n",
        "                features.append(mfcc)\n",
        "                labels.append(genres.index(genre))\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error loading {file_path}: {e}\") # Changed print format\n",
        "\n",
        "print(f\"Finished loading features. Found {len(features)} feature sets and {len(labels)} labels.\") # Debugging print\n",
        "\n",
        "# --- Step 2: Pad sequences to same length ---\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "if len(features) == 0:\n",
        "    raise ValueError(\"No audio features loaded. Please check the dataset directory and file formats.\")\n",
        "\n",
        "maxlen = max(f.shape[0] for f in features) if features else 0 # Calculate maxlen from loaded features\n",
        "\n",
        "features = pad_sequences(features, maxlen=maxlen, dtype='float32', padding='post')\n",
        "labels = to_categorical(labels, num_classes=len(genres)) # Added num_classes\n",
        "\n",
        "print(f\"‚úÖ Data loaded and padded successfully! Features shape: {features.shape}, Labels shape: {labels.shape}\") # Debugging print\n",
        "\n",
        "# --- Step 3: Split Data ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Step 4: Build CNN + LSTM Model ---\n",
        "model = Sequential()\n",
        "model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(len(genres), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# --- Step 5: Train Model ---\n",
        "print(\"üöÄ Training CNN + LSTM model...\") # Debugging print\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=8, validation_data=(X_test, y_test), verbose=1) # Reduced epochs for faster execution\n",
        "\n",
        "# --- Step 6: Evaluate ---\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"üéØ Test Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# --- Step 7: Save Model (Optional) ---\n",
        "# model.save(\"/content/song_cnn_lstm_model.h5\")\n",
        "# print(\"üíæ Model saved successfully at /content/song_cnn_lstm_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7SSR5p5maCsI",
        "outputId": "06620587-3339-4112-d503-0ad9a2eae6d5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: /content/audio_dataset\n",
            "Found genres: ['rap', 'melody']\n",
            "Checking directory: /content/audio_dataset/rap\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [00:03<00:00,  6.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking directory: /content/audio_dataset/melody\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing melody: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.52it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished loading features. Found 41 feature sets and 41 labels.\n",
            "‚úÖ Data loaded and padded successfully! Features shape: (41, 1292, 40), Labels shape: (41, 2)\n",
            "üöÄ Training CNN + LSTM model...\n",
            "Epoch 1/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 473ms/step - accuracy: 0.4875 - loss: 0.7056 - val_accuracy: 0.8889 - val_loss: 0.5465\n",
            "Epoch 2/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 354ms/step - accuracy: 0.6583 - loss: 0.6204 - val_accuracy: 0.7778 - val_loss: 0.5003\n",
            "Epoch 3/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 408ms/step - accuracy: 0.8333 - loss: 0.4494 - val_accuracy: 0.7778 - val_loss: 0.4786\n",
            "Epoch 4/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 360ms/step - accuracy: 0.8833 - loss: 0.4283 - val_accuracy: 0.7778 - val_loss: 0.4945\n",
            "Epoch 5/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354ms/step - accuracy: 0.7375 - loss: 0.3828 - val_accuracy: 0.7778 - val_loss: 0.4715\n",
            "Epoch 6/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 470ms/step - accuracy: 0.6792 - loss: 0.5289 - val_accuracy: 0.6667 - val_loss: 0.4834\n",
            "Epoch 7/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 369ms/step - accuracy: 0.9000 - loss: 0.2838 - val_accuracy: 0.7778 - val_loss: 0.4641\n",
            "Epoch 8/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 354ms/step - accuracy: 0.7708 - loss: 0.4063 - val_accuracy: 0.7778 - val_loss: 0.4161\n",
            "Epoch 9/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 610ms/step - accuracy: 0.9333 - loss: 0.2221 - val_accuracy: 0.8889 - val_loss: 0.3699\n",
            "Epoch 10/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 450ms/step - accuracy: 0.8292 - loss: 0.2739 - val_accuracy: 0.8889 - val_loss: 0.3672\n",
            "Epoch 11/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 358ms/step - accuracy: 0.9792 - loss: 0.1849 - val_accuracy: 0.8889 - val_loss: 0.4199\n",
            "Epoch 12/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 355ms/step - accuracy: 0.8375 - loss: 0.3498 - val_accuracy: 0.6667 - val_loss: 0.4996\n",
            "Epoch 13/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 628ms/step - accuracy: 0.8875 - loss: 0.2539 - val_accuracy: 0.7778 - val_loss: 0.5543\n",
            "Epoch 14/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 438ms/step - accuracy: 0.8417 - loss: 0.3261 - val_accuracy: 0.7778 - val_loss: 0.6220\n",
            "Epoch 15/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 353ms/step - accuracy: 0.9667 - loss: 0.1840 - val_accuracy: 0.6667 - val_loss: 0.6259\n",
            "Epoch 16/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356ms/step - accuracy: 0.9750 - loss: 0.1214 - val_accuracy: 0.6667 - val_loss: 0.6611\n",
            "Epoch 17/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 356ms/step - accuracy: 0.9667 - loss: 0.1110 - val_accuracy: 0.7778 - val_loss: 0.6347\n",
            "Epoch 18/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 355ms/step - accuracy: 0.9292 - loss: 0.1368 - val_accuracy: 0.6667 - val_loss: 0.5993\n",
            "Epoch 19/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 597ms/step - accuracy: 0.9417 - loss: 0.1723 - val_accuracy: 0.6667 - val_loss: 0.6087\n",
            "Epoch 20/20\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 413ms/step - accuracy: 0.9417 - loss: 0.1828 - val_accuracy: 0.6667 - val_loss: 0.6141\n",
            "üéØ Test Accuracy: 66.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model.save('cnn_lstm_audio_model.h5')\n",
        "\n",
        "# Load model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('cnn_lstm_audio_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1NqrOWha0Kv",
        "outputId": "e2464e88-f00d-41c1-81df-383d1e8dd811"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_genre(file_path):\n",
        "    y, sr = librosa.load(file_path, duration=30)\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T\n",
        "    mfcc = pad_sequences([mfcc], maxlen=1292, dtype='float32', padding='post') # Corrected maxlen\n",
        "    pred = model.predict(mfcc)\n",
        "    genre_idx = np.argmax(pred)\n",
        "    return genres[genre_idx]\n",
        "\n",
        "# Example\n",
        "print(predict_genre(\"/content/audio_dataset/rap/Kappe-Varroh.mp3\")) # Corrected file path\n",
        "print(predict_genre(\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TunaXDLja3QP",
        "outputId": "d0731313-c781-42eb-a981-804c50cddde7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
            "rap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2HMqYraZcLbv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}